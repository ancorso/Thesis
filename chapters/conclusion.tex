This chapter summarizes the safety validation techniques developed in this thesis, highlights the contributions, and presents some possible areas of future work.

\section{Summary}
%Introducing safety validation and a short argument for black-box safety validation.
Before safety-critical autonomous systems can be deployed they must undergo rigorous safety validation, but this remains a challenge due to the complexity of the systems and their operational environments. Scenario-based testing can be used to construct a suite of challenging scenarios that an autonomous system must succeed in, but may miss unforeseen or emergent failures. Formal verification can be used to prove the correct operation of system components but cannot scale to complex systems or stochastic environments. Real-world testing is required to to show that implementation and integration has not introduced new failure modes, but is expensive to perform and can be dangerous if the system is not already very safe. 

Black-box sampling approaches address many of the drawbacks to traditional safety validation techniques. The autonomous system is treated as a black box that takes actions in a simulated stochastic environment. Stochastic disturbances in the environment are controlled by an adversary with the goal of causing the system to fail. The adversary relies on sampling to discover failures and can therefore work with complex systems and environments. Machine learning is used to guide the search toward the discovery of unforeseen and complex failures. Black-box sampling has emerged as a promising tool for validating modern autonomous systems but still suffers from several drawbacks including interpretability, scalability and computational expense. 

% Interpretability
Traditional testing techniques such as unit testing or scenario generation usually target specific behaviors or components of the autonomous system. When a failure is found, it is often clear which narrow set of environmental factors led to it. In black-box sampling, however, there is usually a large number of stochastic variables being controlled over time, so when a failure is discovered it is not always clear what caused it. We solve this problem by searching for failure descriptions, which are low-dimensional, interpretable descriptions of failures, instead of example failure trajectories. Failure descriptions take the form of signal temporal logic expressions that are optimized using genetic programming. Failure descriptions provide insight into the possible causes of a failure and we can use them to produce many failure examples. Since the failure description usually has a limited number of parameters, we can perform a sensitivity analysis on the safety of the system with respect to disturbance parameters. 

% Estimating the distribution over failures
Black-box sampling techniques can be used to estimate the probability of failure of a system and thereby provide probabilistic guarantees of safety. The traditional technique of estimating the probability of rare events is importance sampling, which iteratively learns the optimal distribution over disturbance trajectories. Learning a distribution over a high-dimensional trajectory, however, is computationally challenging and may requires many samples. To mitigate this problem, we learn a policy that maps environment states to disturbances that cause the system to fail. The policy can be learned using reinforcement learning algorithms and results in a sampling distribution that is effective for estimating the probability of failure. 

% Scene decomposition
The use of the environment state for discovering failures can improve performance, but limits the scalability safety validation algorithms when the state space is large, such as in multi-agent settings. To improve scalability for multi-agent systems, we decompose the problem into smaller safety validation problems between the system and each other agent. Each subproblem is solved and the results are combined using a neural network trained on rollouts of the full problem. We show improved rates of discovered failures and better estimation of the probability of failure using decomposition. 

% Iterative safety validation
During the development of autonomous systems, safety validation is often performed many times on closely related systems. The system may be improving over time as it is developed, or several competing systems may be compared. Despite our described improvements in scalability, black-box safety validation still requires many samples to reliably discover failures, so frequent safety validation imposes a large computation burden. To reduce this expense, we transfer knowledge in the form of value functions from the safety validation of previous systems to later systems. The value functions are combined with a learned set of attention weights that allows for more efficient safety validation on new systems. 

\section{Contributions}
This work attempts to address several of the existing challenging for validating safety-critical autonomous systems including scalability, interpretability, and efficiency. In pursuit of these goals we made the following contributions:

\paragraph{A model for black-box safety validation of autonomous systems.} In \cref{ch2} we presented a general model of black-box safety validation that describes a system taking actions in a stochastic environment with disturbances controlled by an adversary. This model allowed for the identification of three safety validation tasks and the classification of existing safety validation algorithms based on optimization, path-planning, reinforcement learning and importance sampling. 

\paragraph{A technique for generating interpretable failure descriptions.} In \cref{ch4} we developed a technique that can discover failure descriptions of an autonomous system in the from of a signal temporal logic specification on the disturbances. Expressions were evaluated on their ability to produce failure examples and optimized using genetic programming. Failure descriptions provide insight into why the system failed and can produce many failure examples for further analysis or training. The failure descriptions often had a small number of parameters and could be used to perform a sensitivity analysis on the safety of the system. 

\paragraph{A state-dependent importance sampling distribution for approximating the distribution over failures.} In \cref{ch5} we proposed a state-dependent sampling distribution that could be used efficiently estimate the probability of failure. The proposed distribution chooses disturbances proportional to the probability that the disturbance will lead to failure, which is shown to be optimal for deterministic environments. We present several techniques for estimating the probability of failure and show that the resulting policy is robust to errors in that estimate. 

\paragraph{A problem decomposition technique to improve the scalability of safety validation algorithms for multi-agent systems.} In \cref{ch6} we propose a decomposition approach to scale state-dependent safety validation algorithms to multi-agent systems with large state spaces. When the problem is decomposed into pairwise interactions between the system and each adversary it may be tractable to solve. The solutions to these subproblems are combined with a learned set of weights and help solve the full problem efficiently. 

\paragraph{A transfer learning approach for improving the efficiency and performance of safety validation of multiple related systems.} In \cref{ch7} we present a transfer-learning technique to transfer knowledge in the form of value functions from previous safety validation tasks to new tasks. The transfer allows for better initial and final performance of the safety validation algorithm as well as better training efficiency. 

\section{Future Work}
The algorithms presented in this thesis take a step toward scalable and interpretable safety validation for black-box autonomous systems but further research is required before applying these approaches confidently to safety-critical applications. This section outlines some possible future research directions that can expand the capability and utility of safety validation algorithms. 

\paragraph{Applications to industrial driving policies and real world data distributions.} The safety validation algorithms discussed in this thesis were tested on relatively simple 2D driving simulators with rule-based autonomy. It remains an open question how these algorithms will work when used with a more realistic driving environment and more advanced policies. Additionally, we use simplistic models of environment disturbances but it is possible to construct probabilistic behavior models from real world data~\cite{ellis2009modelling}. Future work should combine realistic simulators with disturbance models generated from real-world data. Large scale simulators that render 3D graphics and vehicle dynamics are much more costly to simulate so data efficiency will be of critical importance. The simulators may also be very complex which could limit how agents in the scene are controlled, but also provide a vast number possible parameters to use as disturbances. More work needs to be done to identify which disturbances are the most useful for causing failures. 

\paragraph{Adversarial testing with perceptual inputs.} Current work emphasizes the use of disturbances that represent high level perturbations to the system (such as the absolute noise in the detection of a pedestrian's position). Many autonomous systems use perceptual inputs for mapping their environment when are subject to adversarial examples under perturbations to pixel values~\cite{sitawarin2018deceiving}. Recent work~\cite{julian2020validation} combines black box adversarial testing with formal verification to find sequences of image perturbations that cause a system to fail. The limitation of this work is the requirement for small images and small neural networks. Future work should investigate how to scale up the formal verification procedure, or develop other techniques to construct sequences of adversarial images to find failures. 


\paragraph{Policy-gradient methods for estimating the distribution over failures.} Our proposed technique in \cref{ch5} constructs the distribution over failures by estimating the probability of failure at each state. We run into problems when the disturbance space is continuous or when the probability of failure is difficult to represent using a neural network. In reinforcement learning, continuous actions can be used by representing the policy instead of the value function. By analogy, we could investigate the use of neural network policies to directly approximate the distribution over failures. Challenges would include the choice of cost function and network architecture to best represent the distribution over failures. 

\paragraph{Continual learning for safety validation algorithms.} Although we have demonstrated performance and efficiency improvements by applying transfer learning to iterative safety validation, our approach is limited if the number of systems grows too large. Since we explicitly store and reference the safety validation policies from previous tasks, the complexity of the algorithm increases over time. The field of \emph{continual learning} tries to address these problems by developing algorithms that can retain competency on previous tasks without explicitly storing previous versions of a policy. Future work on iterative safety validation will include the application of continual learning algorithms to create adversaries that do not forget how to induce previously discovered failure modes. 