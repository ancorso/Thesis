% Intro the problem of safety validation
Introducing autonomy into safety-critical domains, such as autonomous driving, aviation, and medicine, has the potential to improve both safety and efficiency. The consequences of operational errors of these systems include loss of property or human life, so extensive safety validation and testing is required before deployment. Black-box sampling approaches have emerged as a scalable safety validation tool for discovering failures in complex environments. Algorithms based on optimization, path planning, reinforcement learning and importance sampling have been explored in the literature, often with the goal of finding failures of a system with fewer samples or less computational effort. There has been little focus on the potential efficiency of validating many related systems sequentially. 

% Paragraph on the types of problems that could benefit from iterative efficiency
% The design cycle of safety critical systems is iterative. On each iteration, a new prototype of the system is developed and must be validated before deploying or iterating further. This process implies that safety validation algorithms will be used periodically during the design process, and will repeatedly be applied to closely-related systems. Another context in which safety validation may be applied sequentially, is that of a third-party regulatory body that certifies autonomous systems. Such an organization will perform safety validation on systems with similar goals but subtle behavioral differences. Safety validation of complex systems often requires significant computational effort, and most of the existing approaches start from scratch each time the system under test is changed. The need to frequently perform safety validation on related systems therefore imposes a large computational burden, but also an opportunity to improve safety validation efficiency. 

\section{Motivation}
Designing and certifying safety-critical systems generally involves assessing a sequence of closely related systems. Safety validation of complex systems often requires significant computational effort and existing approaches generally start from scratch each time the system under test is changed. The need to frequently perform safety validation on related systems therefore imposes a large computational burden, but also an opportunity to improve safety validation efficiency. 

% What we did
To improve the efficiency of safety validation across related systems, we use knowledge from the validation of previous systems to inform the validation of the next system. We formulate iterative safety validation as a transfer learning problem by modeling each safety validation task as a Markov decision process. The previously solved safety validation problems are used as the set of source tasks, and we transfer knowledge to future tasks in the form of action value functions. We use a set of state-dependent attention weights to automatically learn which previous solutions are applicable to the current problem.

Existing safety validation algorithms use approaches from optimization~\cite{Mathesen2019falsification}, path-planning~\cite{zutshi2014multiple}, reinforcement-learning~\cite{lee2015adaptive}, and importance sampling~\cite{huang2017accelerated}, and often only address the validation of a single system. \\Textcite{uesato2019rigorous} use previous versions of a system to train a failure classifier that predicts which initial conditions of a system will lead to failure, but their approach is not applicable to sequential decision making problems of the type we consider. \\Textcite{wang2020falsification} alternately train an agent and perform safety validation on it to improve robustness. The safety validation algorithm warm starts with the parameters from the previous iteration to improve efficiency. In fact, any parametric safety validation algorithm~\cite{koren2018adaptive, Akazaki2018falsification, kim2016improving} could simply reuse parameters from previous tasks and then fine-tune them for better performance. We demonstrate in our experiments that a fine-tuning approach may work when the systems are very similar, but has poor performance when systems exhibit different behavior.

We propose a new variation of the attend, adapt, and transfer (A2T) algorithm~\cite{rajendran2017attend} that transforms the state and action value spaces for each source task to increase knowledge transfer between dissimilar tasks and compare it to basic A2T and a fine-tuning baseline. We evaluate the initial performance, the final performance, and the number of training steps required to reach the same performance as a no-transfer algorithm. We consider four iterative safety validation tasks in gridworld and autonomous driving scenarios and demonstrate that transfer learning has the potential to significantly improve the performance and sample efficiency of safety validation algorithms.


\section{Background}
This section provides the necessary background for applying transfer learning to safety validation. We introduce Markov decision processes (MDPs), discuss ways to share knowledge between related MDPs, and formulate safety validation as a sequence of MDPs.

\subsection{Transfer Learning}
% What is transfer learning for RL?
Transfer learning is concerned with using knowledge gained by solving one task to improve the learning process in another related task~\cite{taylor2009transfer}. In reinforcement learning, each task is an MDP and each solution is a policy. When tasks have different state and action spaces, we require task mappings that relate the states and actions between tasks. Task mappings may be provided by a human~\cite{taylor2007transfer} or learned from data~\cite{taylor2008autonomous}. If the state and action spaces are the same, then tasks can share a variety of low-level information such as experience samples $(s, a, s', r)$, action value functions, policies, or models of the environment. High-level information, such as a set of options, shaping rewards or feature encodings, may also be transferred to improve learning on a new task. One challenge in transfer learning is \emph{negative transfer} where knowledge from one or more source tasks impairs the performance on the current task. Negative transfer can be mitigated using an attention mechanism, or a human oracle that decides which tasks are relevant~\cite{taylor2009transfer}. 

% How are transfer learning algorithms evaluated
Transfer learning algorithms can be evaluated against a no-transfer alternative in a variety of ways (\cref{fig:transfer_metrics}). \emph{Jumpstart} is the amount of improvement before any training has occurred, \emph{final performance} is the difference between the best performances achieved, \emph{total reward} is the area under the training curve,  and \emph{steps to threshold} is the difference between the number of training steps required to reach a specified threshold.

\begin{figure}
\centering
\includegraphics[width=0.8\linewidth]{figures/iterative_validation/transfer_metrics}
\caption{Metrics for evaluating transfer learning algorithms. }
\label{fig:transfer_metrics}
\end{figure}

% How is safety validation a transfer learning problem
Safety validation MDPs may differ in a variety of ways, which we can explain using an example based on autonomous driving. If validation is performed on two different road geometries (e.g. highway-driving  and an intersection), then the state space and disturbance space may be different. If we fix the road geometry, then the transition model may vary if we validate different driving policies. The reward function will vary if the disturbance model changes or we alter the set of failure states. In this work, we consider systems with different behavior operating in similar environments. We, therefore, assume that the state space, disturbance space, and reward function remain fixed while the transition model varies between tasks.

\section{Proposed Approach}
This section describes our approach for improving iterative safety validation using transfer learning. We first show how to formulate iterative safety validation as a sequence of tasks and specify two ways that knowledge transfer may occur. We then introduce A2T as our choice of transfer learning algorithm and propose a modification to it.

\subsection{Problem Formulation}
Suppose we are performing safety validation on a sequence of related systems and  must validate each system before observing the next one. We model this problem as solving a sequence of MDPs (or tasks) $[T_1, T_2, \ldots]$ where the $i$th task is given by $T_i = (\mathcal{S}, \mathcal{X}, P_i, R, \gamma)$. Due to variations in system behavior, the transition model $P_i$ is unique to each task, while $\mathcal{S}$, $\mathcal{X}$, $R$, and $\gamma$ are shared across tasks. We wish to develop a learning algorithm $L$ that solves task $i$ given the $i-1$ previous solutions $[K_1, K_2, \ldots, K_{i-1}]$ such that
\begin{equation}
    K_i = L(T_i; K_{1:i-1}) \text{.}
\end{equation}
The previous solutions may take the form of value functions or policies. The learning procedure is iterative because the new solution $K_i$ can be added to the set of previous solutions when solving the next task $T_{i+1}$. Since all previous solutions are used, $L$ must avoid negative transfer by learning which source task solutions are applicable to the current task.



In the context of safety validation, we hypothesize two qualitatively distinct ways that the tasks will be related. The first case is that of a \emph{learning system} which is improving its performance from task to task. Each task is therefore more challenging for the adversary since failures that were present in previous tasks may no longer exist or may only occur due to a narrower range of disturbances. In this context, the most recent solutions are likely to provide the most relevant information and large parts of those solutions may be directly applicable to the new task. The second case is that of \emph{comparable systems} where the systems have a similar level of competency but exhibit different behavior. Disturbance trajectories that lead to failure for one system may not cause failure in any other systems, although they might share similar conceptual failure modes. In this setting, direct transfer of solutions may be ineffective, and we need to rely on other types of knowledge. 


\subsection{Choice of Learning Algorithm}
\begin{figure}
\centering
\resizebox{0.7\columnwidth}{!}{\input{figures/iterative_validation/A2T_SAT}}
\caption{Architecture of A2T with state and action transformation. The shaded boxes have learnable parameters. }
\label{fig:a2t_architecture}
\end{figure}

To accelerate safety validation we use the attend, adapt, and transfer (A2T) learning algorithm~\cite{rajendran2017attend} with a minor modification. A2T accelerates learning on a new task by combining the solutions of $k$ previous tasks using a learned set of state-dependent attention weights. To avoid negative transfer, A2T simultaneously learns a solution from scratch so there are a total of $k+1$ solutions and as  many attention weights. A2T can be used to estimate optimal policies or optimal value functions and we found it most straightforward to estimate the optimal action value function. When $Q^*$ is estimated by a neural network, called a $Q$-network, the input is a vector representing the state and the output is a vector representing the values of a discrete set of disturbances. To make this clear, if $\mathcal{X} \subseteq \mathbb{R}^m$, then $\hat{Q}(s) \in \mathbb{R}^m$, is a vector that represents the estimates of the optimal values for each disturbance. The action value function is estimated by the expression
\begin{equation}
\hat{Q}(s) = w_0(s) \hat{Q}_{\rm base}(s) + \sum_{i=1}^k w_i(s) \hat{Q}_i(s) \label{eq:A2T}
\end{equation}
where $\hat{Q}_i$ comes from the $i$th source task, $\hat{Q}_{\rm base}$ is learned from scratch and $w_i(s)$ is the $i$th attention weight, normalized so $\sum_{i=0}^m w_i(s) = 1$.


To handle substantially different system behaviors, we propose a modification of A2T where we include a learned transformation of the state and action value function for each of the $k$ previous solutions. If the state space $\mathcal{S} \subseteq \mathbb{R}^n$, then we define a \emph{state transformation} as a function $g_s : \mathbb{R}^n \to \mathbb{R}^n$ and an \emph{action value transformation} as a function $g_x : \mathbb{R}^m \to \mathbb{R}^m$. Applied to the A2T algorithm, the action value estimate with state and action space transformations is given by
\begin{equation}
\hat{Q}(s) = w_0(s) \hat{Q}_{\rm base}(s) + \sum_{i=1}^k w_i(s) g_x^{(i)}(\hat{Q}_i(g_s^{(i)}(s)))
\end{equation}
where $g_s^{(i)}$ and $g_x^{(i)}$ are the state and action value transformations for the $i$th source solution. 



The A2T algorithm with state and action value transformations can be encoded as the network architecture shown in \cref{fig:a2t_architecture}. The state is used as input to the base network, the source solutions, and the attention network. The base network is a $Q$-network that learns from scratch. The $k$ source solutions are the $Q$-networks that represent the optimal solutions of the source tasks. The source solutions are preceded by a state transformation and followed by an action value transformation. These transformations have the same output dimension as input dimension and are initialized to the identity transformation (with a small amount of noise for breaking symmetry). The attention network has $k+1$ output units with a softmax layer for normalization. The $Q$ values of each source solution and the base network are weighted by the corresponding attention weight and summed together to get a final estimate. The network is trained using the DQN algorithm, which updates the parameters in the base network, the attention network, and the state and action value transformations. If the source solutions are not differentiable with respect to the state (as in the case of a lookup table) then we would apply gradient free optimization~\cite{kochenderfer2019algorithms} to the state transformation.

\begin{figure}
    \centering
    \captionsetup[subfigure]{format=hang}
    \begin{subfigure}[t]{0.25\linewidth}
        \centering
        \includegraphics[width=\textwidth]{figures/iterative_validation/gridworld1.pdf}
        \caption{\scriptsize Source MDP}    
        \label{fig:motiation_source}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.25\linewidth}  
        \centering 
        \includegraphics[width=\textwidth]{figures/iterative_validation/gridworld2.pdf}
        \caption{\scriptsize Locally similar}  
        \label{fig:motivation_a2t}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.25\linewidth}   
        \centering 
        \includegraphics[width=\textwidth]{figures/iterative_validation/gridworld4.pdf}
        \caption{\scriptsize Transformed}
        \label{fig:motivation_sat}
    \end{subfigure}
    \caption{Knowledge transfer scenarios.}
    \label{fig:motivation}
\end{figure}

\vspace{-0.1 in}
\paragraph{Motivation.} Here we provide some intuition for our choice of transfer learning algorithm and the reason for the state and action transformations. Suppose we are solving a sequence of tasks (\cref{fig:motivation}), each is a gridworld where an agent (blue circle) moves between adjacent squares attempting to achieve high reward (green squares) while avoiding states with low reward (red squares). Given the optimal action value function for a source MDP (\cref{fig:motiation_source}), we wish to transfer it to two related tasks. In the first transfer problem (\cref{fig:motivation_a2t}), the reward distribution is locally similar to the source MDP, and only differs in one state. In this case, a policy that works well in the source task will also work well in the new task, especially when the agent is in a state where the local reward landscape matches up (as depicted). A2T will work well in this setting because it can quickly learn attention weights that favor the source policy in most states, and only require our baseline solution when the agent is near the top right corner. In the second transfer problem (\cref{fig:motivation_sat}), A2T is likely to behave poorly because there are no states in which the source policy can be directly applied to achieve high reward. If, however, we could transform the state space by reflecting it across the diagonal and rotate the actions of the agent by \SI{90}{\degree}, then we could directly apply the source policy. This is the motivation for applying transformations both before and after the source solutions. 


\section{Experiments}

This section describes two safety validation problems: a gridworld scenario (GW) and an autonomous driving scenario (AD). Each scenario has two transfer learning problems, one for validating a learning system that improves over time, and the other for validating a set of comparable systems with different behaviors. We then describe the experimental setup and how we compute the evaluation metrics. 

% Gridworld with adversary

\subsection{Gridworld with Adversary}
% \begin{figure}
% \centering
% \includegraphics[width=0.45\linewidth]{figures/iterative_validation/gridworld_with_adversary.pdf}
% \caption{Gridworld with adversary scenario. }
% \label{fig:gridworld_with_adv}
% \end{figure}

% Description of the MDP
The first safety validation problem we model is a gridworld with two agents, shown in \cref{fig:gridworld_with_adv}. The system is the agent in blue who is trying to arrive at one of the squares with positive reward while avoiding the adversary (orange agent). Both agents are initialized randomly and can move to any adjacent or diagonal state that is not a wall (marked as black). As a safety validation MDP, the state is the grid location of both agents and the disturbances are the actions of the adversary: (up, down, left, right, up right, up left, down right, down left, stay). At each step, the blue agent chooses an action based on its policy and transitions to the appropriate state with probability \num{0.7} and transitions to a random feasible state with probability \num{0.3}.  Meanwhile, the orange agent transitions to the state specified by the disturbance $x$. The episode ends when the blue agent arrives at a square with positive reward or when the two agents collide in the same state, which is a failure of the system. We model each disturbance as equally likely to happen so we give a reward of \num{1} for finding a failure and \num{0} otherwise.  

% Description of the difference between tasks
We design two sets of tasks that correspond to the learning system and comparable systems settings. For the learning system, the blue agent is trained using DQN against an orange agent that behaves randomly. Over $\num{e6}$ training steps, \num{10} versions of the system policy were stored, each with an increasing level of performance. Each safety validation task has the adversary validate an increasingly capable version of the learning system. For the comparable systems setting, each task has a different distribution of reward locations, reward values and location of walls. The system learns an optimal policy using dynamic programming~\cite{dmubook}, assuming the adversary behaves randomly. Each system is therefore equally competent, but some configurations of the gridworld are more challenging than others.


% Autonomous driving with blind spot
\subsection{Autonomous Vehicle}
\begin{figure}
\centering
\includegraphics[trim={0 7cm 10cm 6.5cm},clip, width=0.7\linewidth]{figures/iterative_validation/blindspot.pdf}
\caption{Autonomous driving scenario}
\label{fig:av_blindspot}
\end{figure}

% Description of the MDP
The second safety validation problem we model is an autonomous vehicle navigating an intersection with a crossing pedestrian, shown in \cref{fig:av_blindspot}. The system is a vehicle controlled by the intelligent driver model (IDM)~\cite{kesting2010enhanced}, a rule-based driving policy that avoids collisions, and we add a blind spot with a specified direction and angular width. The vehicle tries to reach the end of the road while yielding to the pedestrian. Both agents are initialized randomly in a starting range of initial conditions. 

As a safety validation MDP, the state is the position and velocity of the agents and the disturbances are accelerations of the pedestrian. The pedestrian can accelerate up, down, left, or right by \SI{1}{m/s^2} with probability \num{0.01} or have no acceleration with probability \num{0.96}. The velocity of the pedestrian is limited to an absolute value of \SI{3}{m/s}. At each step, the vehicle chooses an acceleration based on the IDM rules and the location of the pedestrian.  The pedestrian acceleration is specified by the choice of disturbance. Both agents have their position and velocity updated deterministically from their current state and acceleration. The episode terminates when the vehicle reaches the end of the road safely or a collision occurs between the pedestrian and the vehicle. The reward function is given by \cref{eq:reward} where the set $E$ defined by any state where the vehicle and pedestrian overlap.  

% \begin{table}
% \small
%     \centering
%     \caption{Disturbance space for the pedestrian.}
%     \label{tab:accelerations}
%     \begin{tabular}{@{}lrr@{}} 
%         \toprule
%         \textbf{Disturbance} & \textbf{Acceleration} $(a_x, a_y)$ & \textbf{Probability} \\
%         \midrule
%         No disturbance &  (\SI{0}{m/s^2},  \SI{0}{m/s^2}) & \num{0.96}\\
%         Accelerate right & (\SI{1}{m/s^2},  \SI{0}{m/s^2}) & \num{0.01}\\
%         Accelerate left & (\SI{-1}{m/s^2},  \SI{0}{m/s^2}) & \num{0.01}\\
%         Accelerate up & (\SI{0}{m/s^2},  \SI{1}{m/s^2}) & \num{0.01}\\
%         Accelerate down & (\SI{0}{m/s^2},  \SI{-1}{m/s^2}) & \num{0.01}\\
%         \bottomrule
%     \end{tabular}
% \end{table}

% Description of the difference between tasks
For the autonomous driving scenario, we also design two sets of tasks corresponding to a learning system setting and a comparable systems setting. Since the autonomous vehicle does not use machine learning, we simulate an improvement by progressively shrinking the blind spot of the vehicle. The blind spot remains in the same direction (\SI{20}{\degree} from the horizontal), but reduces in width from \SI{30}{\degree} to \SI{6}{\degree} over \num{10} iterations. The vehicle therefore has a decreasing rate of failures over the tasks but there is some overlap in failure modes between adjacent tasks. For the comparable systems setting, each system has a blind spot sampled uniformly at random with a direction in the range $[\SI{-30}{\degree}, \SI{30}{\degree}]$ and an angular width in the range $[\SI{3}{\degree}, \SI{9}{\degree}]$. From a population of \num{30} tasks, we selected \num{9} tasks that differed substantially, to make the transfer problem as challenging as possible within our setting. Two tasks differed if the optimal safety validation policy of one performs poorly on the other. 


\subsection{Experimental Setup}

The experimental procedure is as follows. For each task in a set of tasks, we solve for an optimal $Q$-network from scratch using DQN with prioritized replay, double $Q$-learning~\cite{Hasselt2016deep}, and the Huber loss. We construct a learning curve during training by periodically storing the evaluation of the $Q$-network. For the second task onward, we then solve it using the same learning algorithm with the following $Q$-network  architectures:
\begin{itemize}
    \item \textbf{Fine-tune}: Train the last layer of the previous $Q$-network.
    \item \textbf{A2T}: A2T architecture with previous $Q$-networks as the source solutions.
    \item \textbf{A2T+SAVT}:  A2T architecture augmented with linear state and action value transformations.
\end{itemize}
The networks are initialized with Xavier initialization~\cite{glorot2010understanding}, while the transformations were initialized to the identity matrix with uniform random noise  in the range [-\num{1e-3}, \num{1e-3}] added to the parameters to break symmetry. Additional information on network architecture and hyperparameters is shown in \cref{tab:hyperparameters}.


\begin{table}
    \centering
    \caption{Network architectures and hyperparameters. }
    \label{tab:hyperparameters}
    \begin{tabular}{@{}ll@{}} 
        \toprule
        \textbf{Parameter} & \textbf{Value}  \\
        \midrule
        Base network & 3 hidden layers, [\num{64}, \num{32}, \num{16}] units\\
        Attention network & 1 hidden layer, \num{16} units \\
        Activation function & relu \\
        State/Action transform & Linear \\
        Training steps & \num{3e6} \\
        Batch size & \num{64} \\
        Learning rate $\alpha$ & \num{4e-5} (GW), \num{5e-5} (AD) \\
        Target update frequency & \num{2000} (GW), \num{3000} (AD) \\
        Evaluation frequency & \num{2000} \\
        No. evaluation episodes  & \num{300} \\
        Exploration policy & $\epsilon$-greedy with $\epsilon \in [1, 0.1]$\\
        \bottomrule
    \end{tabular}
\end{table}

% Processing the learning curves
We filter each learning curve using a moving-average filter with a width of \num{20} evaluations steps to help remove the noise due to finite sample evaluation and any outlier evaluation points. For each learning curve we identify the \emph{near-optimal} performance as $\mu - \sigma$, where $\mu$ and $\sigma$ are the mean and standard deviation of the performance in a window with a width of \num{100} evaluation steps around the point of maximum performance. We use near-optimal performance because it is a more stable measure of how fast the learning took place than the point of maximum performance.

% computing evaluation metrics
The jumpstart is the difference in initial performance between a transfer and no-transfer learning algorithms. It can be computed from the first entries in the learning curves. When reporting jumpstart, we only include fine-tuning and A2T because A2T+SAVT has the same outputs as A2T until the transformations deviate from identity. The final performance is the difference in near-optimal performance between the transfer and no-transfer learning algorithms. The steps to threshold metric measures how many training steps are required for a transfer learning algorithm to reach the near-optimal performance of the no-transfer algorithm.

% normalization of metrics. 
The metrics are normalized with reference to the learning curve of the no-transfer algorithm because the initial and near-optimal performance varies between tasks. Let $y$ be the performance (initial or final) of a transfer learning algorithm and $y_{\rm ref}$ be the performance of the no-transfer learning algorithm, then we report the fractional difference in performance $(y - y_{\rm ref})/|y_{\rm ref}|$. Let $t$ be the number of training steps required to reach a threshold for a transfer learning algorithm and $t_{\rm ref}$ be the same quantity for the no-transfer learning algorithm, then we report the ratio $t / t_{\rm ref}$.

% Notes
We use the number of training steps rather than the wall clock time because we assume that the cost of running the simulator is much larger than the cost of updating the parameters of the model. This is a good assumption for high-fidelity simulators that are often used for validating safety-critical systems. We also assume that the training time of previous source tasks is a sunk cost and it is not included in our efficiency metric. This assumption is valid in the case of iterative safety validation because the new version of the system must be validated regardless of the approach used. When using A2T in a real-world setting, we would not solve each task from scratch and therefore the source solutions would take the form of A2T networks. A practitioner may wish to compress the A2T network~\cite{julian2019deep} into a traditional architecture before it is used as a source solution. We chose to use the networks trained from scratch for ease of implementation and to isolate the effects of transfer learning from other issues.


\section{Results and Discussion}
\begin{figure*}
    \centering
    \begin{subfigure}[b]{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/iterative_validation/gridworld_learning/jumpstart.pdf}
        \caption{Jumpstart fractional improvement.}
        \label{fig:gwl_jumpstart}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/iterative_validation/gridworld_learning/peak_performance.pdf}
        \caption{Final fractional improvement.}
        \label{fig:gwl_final}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/iterative_validation/gridworld_learning/steps_to_threshold.pdf}
        \caption{Step ratio to threshold.}
        \label{fig:gwl_step}
    \end{subfigure}
    \caption{Evaluation metrics for the gridworld scenario with a learning system.}
    \label{fig:gwl}
\end{figure*}



\begin{figure*}
    \centering
    \begin{subfigure}[b]{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/iterative_validation/gridworld_comparison/jumpstart.pdf}
        \caption{Jumpstart fractional improvement.}
        \label{fig:gwc_jumpstart}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/iterative_validation/gridworld_comparison/peak_performance.pdf}
        \caption{Final fractional improvement.}
        \label{fig:gwc_final}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/iterative_validation/gridworld_comparison/steps_to_threshold.pdf}
        \caption{Step ratio to threshold.}
        \label{fig:gwc_step}
    \end{subfigure}
    \caption{Evaluation metrics for the gridworld scenario with comparable systems.}
    \label{fig:gwc}
\end{figure*}




\begin{figure*}
    \centering
    \begin{subfigure}[b]{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/iterative_validation/driving_learning/jumpstart.pdf}
        \caption{Jumpstart fractional improvement.}
        \label{fig:adl_jumpstart}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/iterative_validation/driving_learning/peak_performance.pdf}
        \caption{Final fractional improvement.}
        \label{fig:adl_final}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/iterative_validation/driving_learning/steps_to_threshold.pdf}
        \caption{Step ratio to threshold.}
        \label{fig:adl_step}
    \end{subfigure}
    \caption{Evaluation metrics for the autonomous driving scenario with a learning system.}
    \label{fig:adl}
\end{figure*}




\begin{figure*}
    \centering
    \begin{subfigure}[b]{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/iterative_validation/driving_comparison/jumpstart.pdf}
        \caption{Jumpstart fractional improvement.}
        \label{fig:adc_jumpstart}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/iterative_validation/driving_comparison/peak_performance.pdf}
        \caption{Final fractional improvement.}
        \label{fig:adc_final}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/iterative_validation/driving_comparison/steps_to_threshold.pdf}
        \caption{Step ratio to threshold.}
        \label{fig:adc_step}
    \end{subfigure}
    \caption{Evaluation metrics for the autonomous driving scenario with a comparable systems.}
    \label{fig:adc}
\end{figure*}


We solved each of the four safety validation tasks using 3 transfer learning algorithms and report the evaluation metrics against the task index in \cref{fig:gwl,fig:gwc,fig:adl,fig:adc}. The discussion of the results is grouped by evaluation metric. 


\paragraph{Jumpstart.}
\Cref{fig:gwl_jumpstart,fig:gwc_jumpstart,fig:adl_jumpstart,fig:adc_jumpstart} show the jumpstart of the fine-tune and A2T architectures. We see that across all four safety validation problems, the transfer learning algorithms contributed a significant increase in initial performance. For most tasks, the A2T architecture had slightly better jumpstart than simply reusing the previous solution, especially in \cref{fig:gwc_jumpstart}. The gridworld with comparable systems had substantially different failure modes between tasks and therefore had the least benefit in jumpstart.  The safety validation problems with learning systems had the jumpstart decrease with the number of tasks observed, likely due to the increase in difficulty of the tasks. For the safety validation problems involving comparable systems, however, the jumpstart tended to increase with the number of source tasks. We hypothesize that with more source tasks, we are more likely to have a task that closely matches the current tasks, and can therefore immediately have reasonable performance. 

\paragraph{Final Performance.}
\Cref{fig:gwl_final,fig:gwc_final,fig:adl_final,fig:adc_final} show the final performance of each transfer learning algorithm. The final safety validation performance can be significantly improved by both A2T approaches, but not through fine-tuning. In all but \cref{fig:adl_final}, the fine-tuning approach was not able to match the no-transfer performance given the same number of iterations. This lack of performance could mean that the $Q$-network for one task is not learning a set of features that is useful for solving other tasks, so updating only the final layer does not provide enough capacity to solve the problem.  The A2T networks, however, are able to achieve significantly improved final performance, which generally increases with the number of source tasks. The A2T network with state and action value transformations outperforms the basic A2T network in both safety validation problems with comparable systems, which are the problems it was designed for. Both of the safety validation problems with a learning system show the maximal gain in final performance in the middle of the sequence of tasks. A lower gain in early tasks may be due to those tasks being easy to solve, while a lower gain in the later tasks may be due to only having a few failure modes to exploit. The middle tasks may be challenging to solve but may have a diversity of failure modes that the previous policies can help identify. More experimentation is needed to fully understand these trends. 

\paragraph{Steps to Threshold.}
\Cref{fig:gwl_step,fig:gwc_step,fig:adl_step,fig:adc_step} show the number of training steps required to reach the near-optimal performance of the no-transfer algorithm. In some cases, near-optimal performance is never reached so those data points are omitted from the plots.  We observe that the number of training steps can be reduced by both A2T networks, but in different conditions. The basic A2T network performs well when validating a learning system because parts of previous solutions can be used directly. In \cref{fig:gwl_step}, the number of training steps on some tasks could be reduced by \num{50}\% and in \cref{fig:adl_step} the number of training steps is reduced by more than an order of magnitude in some cases.  

The A2T network with state and action transformations performs slightly worse than the basic A2T network in \cref{fig:gwl_step} and has similar performance in \cref{fig:adl_step}, but significantly outperforms the A2T network for many tasks in the comparable systems setting. In \cref{fig:gwc_step}, the basic A2T network requires more steps than the no-transfer algorithm, which negates the utility of the more complex architecture, while the A2T+SAVT network was able to reduce the number of training steps by up to \num{50}\% on some tasks. We note that generally, the fine-tune approach is unable to achieve the same performance as learning from scratch but when it does reach near-optimal performance, it requires fewer training steps than learning from scratch.

\paragraph{Summary.} From our experiments we conclude that transfer learning can be an effective strategy for improving performance and efficiency of safety validation algorithms. Transfer through fine-tuning can give a significant increase in jumpstart but often fails to reach the level of performance of a $Q$-network trained from scratch. The A2T networks also provides an increase in jumpstart as well as an increase in final performance. The use of a small attention network allows for quick adaptation to new domains as evidenced by the reduction in the number of training steps required to reach near-optimal performance. When the tasks differ significantly from each other, however, the basic A2T network may take longer than the no-transfer algorithm to reach near-optimal performance. We fix this problem by introducing state and action value transformations for each source solution and demonstrate improved training efficiency over the no-transfer algorithm.


\section{Conclusion}
The validation of safety-critical autonomous systems is crucial for their safe deployment. Existing algorithms for validation often start from scratch each time the system changes. The nature of system design implies that safety validation will be performed iteratively on related systems, and should therefore benefit from past experience. We formulate iterative safety validation as a transfer learning problem and demonstrate improvements in both efficiency and performance of transfer learning algorithms compared to a no-transfer baseline. We augmented the attend, adapt, and transfer algorithm with state and action value transformations to allow for more transfer between disparate tasks. We evaluated jumpstart, final performance, and steps to threshold metrics on four iterative safety validation problems in gridworld and autonomous driving domains. Future work will include exploring the failure modes discovered by each algorithm to gain insights into how transfer is occurring. These insights may help us understand under what conditions we can expect performance and efficiency improvements.